#Import Libraries:
import sim                  #V-rep library
import sys
import os
import six.moves.urllib as urllib
import time                #used to keep track of time
import numpy as np         #array library
import math
import matplotlib.pyplot as mlp   #used for image plotting
import cv2 
import tarfile
import tensorflow as tf
import zipfile
import scipy.io
from collections import defaultdict
from io import StringIO
from pylab import *

#from matplotlib import pyplot as plt
from PIL import Image

def BEV (Image, cam_pos):
    width,height = 512,512
    img = Image
    if (cam_pos == 2):
        des = np.float32([[0,0],[width,0],[0,height],[512,height] ])
        src = np.float32([[198, 50], [333, 52], [14, 190], [505, 195]])
    if (cam_pos == 3):
        des = np.float32([[0,0],[width,0],[0,height],[512,height] ])
        src = np.float32([[206, 80], [328, 93], [21, 192], [494, 237]])
    if (cam_pos == 4):
        des = np.float32([[0,0],[width,0],[0,height],[512,height] ])
        src = np.float32([[210, 116], [325, 116], [54, 225], [500, 234]])
    if (cam_pos == 5):
        des = np.float32([[0,0],[width,0],[0,height],[512,height] ])
        src = np.float32([[206, 89], [329, 77], [50, 220], [509, 192]])
    
    M = cv2.getPerspectiveTransform(src, des) # The transformation matrix
    Minv = cv2.getPerspectiveTransform(des, src)
    warped_img = cv2.warpPerspective(img, M, (width, height)) # Image warping
    mlp.imshow(cv2.cvtColor(warped_img, cv2.COLOR_BGR2RGB)) # Show results
    mlp.show()
    
    return warped_img

def Classify (CX, CY):
    if (CX < 256):
        if (CY < 256):
            return "Top-Left"
        else:
            return "Bottom-Left"
    if (CX > 256):
        if (CY < 256):
            return "Top-Right"
        else:
            return "Bottom-Right"
 
def Matching (BEV_list, Obj_list):
    x_val = []
    y_val = []
    pos_BEV = []
    pos_OBJ = []
    name_OBJ = []
    BEV_array = np.asarray(BEV_list)
    OBJ_array = np.asarray(Obj_list)
    detect_list = []

    for idx, i in enumerate (BEV_array):
        length = len(BEV_array[idx])
        j = 0
        while j < length:
            pos_BEV.append(BEV_array[idx][j][0])
            x_val.append(math.ceil((my_array[idx][j][1]) * 100)/ 100)
            y_val.append(math.ceil((my_array[idx][j][2]) * 100)/ 100)
            j += 1
            
            
    for idx, i in enumerate (OBJ_array):
        length = len(OBJ_array[idx])
        j = 0
        while j < length:
            pos_OBJ.append(OBJ_array[idx][j][0])
            name_OBJ.append(OBJ_array[idx][j][1])
            j += 1
            
    len_BEV = len(BEV_array)
    len_OBJ = len(OBJ_array)
    len_FINAL = 0
    if (len_BEV == len_OBJ):
        len_FINAL = len_BEV
    else:
        len_FINAL = len_OBJ
        
    for i in range(0, len_FINAL):
        if (pos_BEV[i] == pos_OBJ[i]):
            detect_list.append([name_OBJ,x_val,y_val])
            
    
    return detect_list
            
    
def Bounding (Image, cam_pos):
    image = cv2.pyrDown((Image))
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) 
    gray = cv2.medianBlur(gray, 15)
    edged = cv2.Canny(gray, 30, 200) 
    contours, hierarchy = cv2.findContours(edged, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    #print("Number of Contours found = " + str(len(contours))) 
    #pair_x = []
    #pair_y = []
    pairs = []
   
    count = 0
    for c in contours:
        # get the bounding rect
        area = cv2.contourArea(c)
        #print(area)
        if (area > 1):
            x, y, w, h = cv2.boundingRect(c)
            # draw a green rectangle to visualize the bounding rect
            cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)
            cX = round(int(x + int(w/2))) #x-coordinate of rectangle's center
            cY = round(int(y + int(h) )) #y-coordinate of rectangle's center + int(h/2)
            
            # Up sampling the centre value by multiplying with 2 samples
            # finding the value from the camera origin (256,512)
            # dividing it with scaling factor to get the original size in meters
            cX = cX * 2
            cY = cY * 2
            #print (cX)
            if (cam_pos == 2):
                if (cX > 256):
                    cX_p = cX - 256
                    x_real = cX_p/500
                if (cX < 256):
                    cX_p = 256 - cX
                    x_real = (cX_p/500) * (-1)
                
                # performing similar in Y valuesa s well
                pos = Classify(cX, cY)
                cY = 512 - cY
                #y_real = cY/500
                y_real = cY/264
                y_real = y_real + 0.5 # offset from the camera to the image point
                
                pairs.append([pos,x_real, y_real])
            
            if (cam_pos == 3):
                if (cX > 256):
                    cX_p = cX - 256
                    x_real = cX_p/500
                    pos = Classify(cX, cY)
                    cY = 512 - cY
                    y_real = cY/264
                    y_real = (y_real + 0.5) * (-1)
                if (cX < 256):
                    cX_p = 256 - cX
                    x_real = (cX_p/500)
                    pos = Classify(cX, cY)
                    cY = 512 - cY
                    y_real = cY/264
                    y_real = y_real + 0.5
            
                pairs.append([pos,x_real, y_real])
                
            
            if (cam_pos == 4):
                if (cX > 256):
                    cX_p = cX - 256
                    x_real = (cX_p/500) * (-1)
                    pos = Classify(cX, cY)
                    cY = 512 - cY
                    y_real = cY/264
                    y_real = (y_real + 0.5) * (-1)
                if (cX < 256):
                    cX_p = 256 - cX
                    x_real = (cX_p/500)
                    pos = Classify(cX, cY)
                    cY = 512 - cY
                    y_real = cY/264
                    y_real = (y_real + 0.5) * (-1)
            
                pairs.append([pos,x_real, y_real])
                
                
            if (cam_pos == 5):
                if (cX > 256):
                    cX_p = cX - 256
                    x_real = (cX_p/500) * (-1)
                    pos = Classify(cX, cY)
                    cY = 512 - cY
                    y_real = cY/264
                    y_real = (y_real + 0.5) 
                if (cX < 256):
                    cX_p = 256 - cX
                    x_real = (cX_p/500) * (-1)
                    pos = Classify(cX, cY)
                    cY = 512 - cY
                    y_real = cY/264
                    y_real = (y_real + 0.5) * (-1)
            
                pairs.append([pos,x_real, y_real])
                
                
                
                
                
                
                
            count = count +1
            # get the min area rect
            rect = cv2.minAreaRect(c)
            box = cv2.boxPoints(rect)
            # convert all coordinates floating point values to int
            box = np.int0(box)
            # draw a red 'nghien' rectangle
            #img = cv2.drawContours(image, [box], 0, (0, 0, 255))
            
    cv2.drawContours(image, contours, -1, (0, 255, 0), 1) 
    #print("Number of Contours modified = " + str(count))
    mlp.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) # Show results
    mlp.show()
    return pairs

    
def obj_image_test(Image):
    cap = cv2.rotate(Image, cv2.ROTATE_180)
    from object_detection.utils import label_map_util

    from object_detection.utils import visualization_utils as vis_util
    # What model to download.
    MODEL_NAME = 'Cuboid_graph'
    
    # Path to frozen detection graph. This is the actual model that is used for the object detection.
    PATH_TO_CKPT ='C:/Custom_Tensorflow2/models/object_detection/Cuboid_graph/frozen_inference_graph.pb'
    
    # List of the strings that is used to add correct label for each box.
    PATH_TO_LABELS = 'C:/Custom_Tensorflow2/models/object_detection/training/labelmap.pbtxt'
    NUM_CLASSES = 1
    # ## Load a (frozen) Tensorflow model into memory.


    detection_graph = tf.Graph()
    with detection_graph.as_default():
      od_graph_def = tf.GraphDef()
      with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')
        
        # PATH_TO_LABELS 

    label_map = label_map_util.load_labelmap(os.path.join('C:/Custom_Tensorflow2/models/object_detection/training', 'labelmap.pbtxt'))
    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
    category_index = label_map_util.create_category_index(categories)
    
    # ## Helper code


    def load_image_into_numpy_array(image):
      (im_width, im_height) = image.size
      return np.array(image.getdata()).reshape(
          (im_height, im_width, 3)).astype(np.uint8)
  
    
    
    with detection_graph.as_default():
        with tf.Session(graph=detection_graph) as sess:
    
            image_np = cap;
            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
            image_np_expanded = np.expand_dims(image_np, axis=0)
            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
            # Each box represents a part of the image where a particular object was detected.
            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
            # Each score represent how level of confidence for each of the objects.
            # Score is shown on the result image, together with the class label.
            scores = detection_graph.get_tensor_by_name('detection_scores:0')
            classes = detection_graph.get_tensor_by_name('detection_classes:0')
            num_detections = detection_graph.get_tensor_by_name('num_detections:0')
            # Actual detection.
            (boxes, scores, classes, num_detections) = sess.run(
                [boxes, scores, classes, num_detections],
                feed_dict={image_tensor: image_np_expanded})
            # Visualization of the results of a detection.
            vis_util.visualize_boxes_and_labels_on_image_array(
                image_np,
                np.squeeze(boxes),
                np.squeeze(classes).astype(np.int32),
                np.squeeze(scores),
                category_index,
                use_normalized_coordinates=True,
                line_thickness=8)
            width_pixels = 0;
            height_pixels = 0;
            (frame_height, frame_width) = cap.shape[:2]
              # Generating x and y positions
            for i,b in enumerate(boxes[0]):
                
                ymin = int((np.squeeze(boxes)[i][0]*frame_height))
                #print(ymin)
                xmin = int((np.squeeze(boxes)[i][1]*frame_width))
                ymax = int((np.squeeze(boxes)[i][2]*frame_height))
                xmax = int((np.squeeze(boxes)[i][3]*frame_width))
                if( int(classes[0][i] == 1)):
                    mid_x = ((boxes[0][i][1]+boxes[0][i][3])/2)*512
                    #print("Object = " + str(mid_x))
                    mid_y = ((boxes[0][i][2]+ boxes[0][i][0])/2)*512
                    #print("Object = " + str(mid_y))
                    mlp.imshow(cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)) # Show results
                    mlp.show()
                    
                    return mid_x, mid_y
                else:
                    return 0,0
                    
                
                
                
                
                #Pre-Allocation

PI=math.pi  #pi=3.14..., constant

sim.simxFinish(-1) # just in case, close all opened connections

clientID=sim.simxStart('127.0.0.1',19999,True,True,5000,5)

if clientID!=-1:  #check if client connection successful
    print ('Connected to remote API server')
    
else:
    print ('Connection not successful')
    sys.exit('Could not connect')

# retrive vision sensor handle
errorCode, cam_Handle = sim.simxGetObjectHandle(clientID,'cam_main',sim.simx_opmode_oneshot_wait)
errorCode,resolution,image=sim.simxGetVisionSensorImage(clientID, cam_Handle,0,sim.simx_opmode_streaming)
errorCode,resolution,image=sim.simxGetVisionSensorImage(clientID, cam_Handle,0,sim.simx_opmode_buffer)

# retrive yaw and pitch handle
errorCode, yaw_Handle = sim.simxGetObjectHandle(clientID,'yaw',sim.simx_opmode_oneshot_wait)
errorCode, pitch_Handle = sim.simxGetObjectHandle(clientID,'pitch',sim.simx_opmode_oneshot_wait)

# roating yaw to capture the images on each angle

cam_start0, cam_start1 = sim.simxGetJointPosition(clientID, yaw_Handle, sim.simx_opmode_oneshot_wait)

angle = 0
errorCode = sim.simxSetJointPosition(clientID, yaw_Handle, angle*math.pi/180, sim.simx_opmode_oneshot_wait)
vrep_data_x=[]
vrep_data_y=[]
real_coordinate = []
for cam_start1 in range(1,6) :
   errorCode = sim.simxSetJointPosition(clientID, yaw_Handle, angle*math.pi/180, sim.simx_opmode_oneshot_wait)
   errorCode,resolution,image=sim.simxGetVisionSensorImage(clientID, cam_Handle,0,sim.simx_opmode_buffer)
   im = np.array (image, dtype=np.uint8)
   im.resize([resolution[0],resolution[1],3])
   im = cv2.rotate(im, cv2.ROTATE_180)
   im = cv2.flip(im, 1)
   mlp.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)) # Show results
   mlp.show()
   if cam_start1 != 1:
       
       #mid_x, mid_y = obj_image_test(im)
       top_view = BEV(im, cam_start1)
       BEV_coor = Bounding(top_view, cam_start1)
       real_coordinate.append(BEV_coor)
       #print(cam_start1)
       #print(coor)
       
   #print(img_series)
   angle = angle-90
   
# Plotting the cartesian coordinate system havinf the rover as origin
x_plot = []
y_plot = []
pos_BEV = []
my_array = np.asarray(real_coordinate)

for idx, i in enumerate (my_array):
    length = len(my_array[idx])
    j = 0
    while j < length:
        pos_BEV.append(my_array[idx][j][0])
        x_plot.append(math.ceil((my_array[idx][j][1]) * 100)/ 100)
        y_plot.append(math.ceil((my_array[idx][j][2]) * 100)/ 100)
        j += 1
    
x = np.asarray(x_plot)
y = np.asarray(y_plot)
fig = plt.figure()
ax = fig.add_subplot(111)

scatter(x,y)

[ plot( [dot_x,dot_x] ,[0,dot_y], '-', linewidth = 0.5 ) for dot_x,dot_y in zip(x,y) ] 
[ plot( [0,dot_x] ,[dot_y,dot_y], '-', linewidth = 0.5 ) for dot_x,dot_y in zip(x,y) ]

left,right = ax.get_xlim()
low,high = ax.get_ylim()
arrow( left, 0, right -left, 0, length_includes_head = True, head_width = 0.02 )
arrow( 0, low, 0, high-low, length_includes_head = True, head_width = 0.02 ) 

grid()

show()

# Transferring the data to matlab by storing it as mat file and then loading the file in matlab
scipy.io.savemat('test.mat', dict(x=x, y=y))
    
 
  
